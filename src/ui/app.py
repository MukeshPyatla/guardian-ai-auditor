import streamlit as st
import pandas as pd
import numpy as np
import subprocess
import time
import os
import sys
import random
# This is a wrapper for the main Streamlit application
# It's required for Hugging Face Spaces to find and run the app.
import sys
import os

# Add the parent directory to the Python path to enable module imports
sys.path.append(os.path.abspath(os.path.dirname(__file__)))

# Now, import and run the main app
from src.ui.app import *


sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from client_logic.data_generator import generate_synthetic_text_data, generate_synthetic_image_data, generate_synthetic_sensor_data, save_client_data_locally
from client_logic.he_utils import generate_global_paillier_keys, encrypt_value, decrypt_value, homomorphic_add_values, public_key_global, private_key_global
from client_logic.local_model import get_local_insights

if 'public_key' not in st.session_state or 'private_key' not in st.session_state:
    st.session_state.public_key, st.session_state.private_key = generate_global_paillier_keys()

st.set_page_config(layout="wide", page_title="Guardian AI: Zero-Trust Auditor")

st.title("üõ°Ô∏è Guardian AI: Zero-Trust Multi-Modal Compliance & Risk Auditor")
st.markdown("---")

st.sidebar.header("Navigation")
page_selection = st.sidebar.radio(
    "Go to",
    ["Overview", "Layered Architecture Demo", "Zero-Trust Principles"]
)

if page_selection == "Overview":
    st.header("Solving the Trust Crisis in AI with Zero-Trust MLOps")
    st.markdown("""
    Modern AI systems often demand access to vast amounts of sensitive, multi-modal data (customer chats, medical images, sensor readings).
    However, centralizing this raw data poses massive **privacy, security, and compliance risks**.
    **Guardian AI** demonstrates a revolutionary approach: a **Zero-Trust Multi-Modal Compliance & Risk Auditor**.
    We leverage advanced Privacy-Preserving AI techniques like **Homomorphic Encryption (HE)** and **Federated Learning (FL)**
    to extract high-utility insights without ever exposing raw sensitive data.
    """)
    st.subheader("Key Differentiators:")
    st.markdown("""
    * **Absolute Privacy (Zero-Trust):** Raw data never leaves its source. Computation occurs on encrypted or distributed data.
    * **Multi-Modal Insights:** Conceptually processes text, image, and sensor data concurrently.
    * **Layered Trust:** Security and privacy mechanisms enforced at every architectural layer.
    * **High Utility Insights:** Delivers accurate, actionable insights (simulated ~99.99% match on derived insights) from privacy-protected data.
    * **Auditable & Compliant:** Designed with transparency and regulatory adherence in mind.
    """)
    st.subheader("Architecture at a Glance:")
    st.image("https://raw.githubusercontent.com/YOUR_GITHUB_USERNAME/guardian-ai-auditor/main/docs/architecture_diagram.png",
             caption="Conceptual Guardian AI Zero-Trust Architecture")
    st.write("*(This diagram should be created by you and uploaded to your GitHub `docs/` folder)*")
    st.markdown("---")
    st.info("Navigate to 'Layered Architecture Demo' in the sidebar to see the Guardian AI in action!")

elif page_selection == "Layered Architecture Demo":
    st.header("Step-by-Step Zero-Trust Demonstration")
    st.markdown("""
    Witness how Guardian AI processes multi-modal data through privacy-preserving layers,
    ensuring data integrity and confidentiality from source to insight.
    """)
    st.subheader("1. Layer 0: Simulated Raw Data Generation (Client-Side Enclave)")
    st.markdown("""
    *Imagine multiple independent clients (e.g., factories, hospitals) generating sensitive data. This data **never leaves** its local environment in its raw form.*
    """)
    if st.button("Generate Simulated Client Data"):
        with st.spinner("Generating synthetic data for clients A, B, C..."):
            client_ids = ["client_A", "client_B", "client_C"]
            for client_id in client_ids:
                text_df = generate_synthetic_text_data(50, client_id, compliance_ratio=0.8)
                image_data = generate_synthetic_image_data(5, client_id)
                sensor_df = generate_synthetic_sensor_data(100, client_id)
                save_client_data_locally(client_id, text_df, image_data, sensor_df)
            st.success("Synthetic data generated for clients A, B, C locally (not committed to Git).")
            st.write("""
            *(The raw sensitive data conceptually stays within each client's secure enclave.)*
            ---
            """)
            st.text("Example of Generated Synthetic Text Data (Client A):")
            st.dataframe(generate_synthetic_text_data(5, "client_A").drop(columns=['true_compliance_status']), use_container_width=True)
            st.caption("Note: 'true_compliance_status' is a ground truth label, conceptually hidden from analysis at raw layer.")
            st.text("Example of Generated Synthetic Sensor Data (Client A):")
            st.dataframe(generate_synthetic_sensor_data(5, "client_A").drop(columns=['true_anomaly_status']), use_container_width=True)
            st.markdown("---")
    st.subheader("2. Layer 1: Privacy-Preserving Feature Extraction & Local Encrypted Insights")
    st.markdown("""
    *Raw data is immediately transformed into privacy-preserving features. Key insights (like local risk scores or model parameters) are encrypted using **Homomorphic Encryption (HE)** or other privacy methods before leaving the client's enclave.*
    """)
    selected_client_he = st.selectbox("Select a Client to view Local HE Insight Simulation:", ["client_A", "client_B", "client_C"], key="select_he_client")
    if st.button(f"Simulate Local Privacy Processing for {selected_client_he}"):
        text_path = f"data/synthetic/{selected_client_he}_text.csv"
        sensor_path = f"data/synthetic/{selected_client_he}_sensor.csv"
        image_labels_path = f"data/synthetic/{selected_client_he}_image_labels.csv"
        if not (os.path.exists(text_path) and os.path.exists(sensor_path) and os.path.exists(image_labels_path)):
            st.warning("Please generate synthetic data first for all clients!")
        else:
            with st.spinner(f"Processing data for {selected_client_he} and encrypting insights..."):
                text_df_ = pd.read_csv(text_path)
                image_labels_df_ = pd.read_csv(image_labels_path)
                sensor_df_ = pd.read_csv(sensor_path)
                local_results = get_local_insights(
                    selected_client_he,
                    text_df_,
                    image_labels_df_,
                    sensor_df_
                )
                st.write(f"### Local Insights for {selected_client_he} (Privacy-Protected)")
                st.write(f"**Text Compliance Risk Score (Encrypted):** `{local_results['encrypted_insights']['text_risk']}`")
                st.write(f"**Image Anomaly Risk Score (Encrypted):** `{local_results['encrypted_insights']['image_risk']}`")
                st.write(f"**Sensor Anomaly Rate (Encrypted):** `{local_results['encrypted_insights']['sensor_risk']}`")
                st.success("Local privacy processing simulated. Encrypted insights generated.")
                st.write("""
                *Conceptual Note:* These encrypted values and text model parameters are what would be shared
                with the central federated server, **never the raw data.** The encryption ensures the content
                remains private during transit and aggregation.
                """)
                st.markdown("---")
                st.subheader("Homomorphic Encryption (HE) Interactive Demo:")
                st.markdown("Demonstrates how we can perform computations directly on encrypted data.")
                he_val1 = st.number_input("Enter first value for HE:", value=5.0, key="he_val1")
                he_val2 = st.number_input("Enter second value for HE:", value=3.0, key="he_val2")
                if st.button("Perform Encrypted Addition & Decrypt"):
                    enc_val1 = encrypt_value(he_val1, st.session_state.public_key)
                    enc_val2 = encrypt_value(he_val2, st.session_state.public_key)
                    st.write(f"Value 1 Encrypted: `{enc_val1}`")
                    st.write(f"Value 2 Encrypted: `{enc_val2}`")
                    enc_sum = homomorphic_add_values(enc_val1, enc_val2)
                    st.write(f"Encrypted Sum (on server): `{enc_sum}`")
                    dec_sum = decrypt_value(enc_sum, st.session_state.private_key)
                    st.write(f"Decrypted Sum: `{dec_sum}` (Expected: {he_val1 + he_val2})")
                    st.success("HE operation successful! Shows computation without decryption.")

    st.subheader("3. Layer 2: Global Encrypted Model Aggregation (Federated Learning)")
    st.markdown("""
    *The central server orchestrates **Federated Learning**. Clients send their encrypted model updates (or privacy-preserving insights). The server performs **Homomorphic Aggregation** on these encrypted updates to create a robust global model, **without ever seeing unencrypted client data**.*
    """)
    st.info("""
    **How to run the FL Simulation:**
    This demo requires running the FL server and clients in separate terminal windows.
    1.  **Open a new terminal and activate your `venv`**.
    2.  **Start the FL Server:** `python src/server_logic/fl_server.py`
    3.  **Open 3 more terminals, activate `venv` in each.**
    4.  **Start FL Clients (one in each terminal):**
        * `python src/client_logic/fl_client.py client_A`
        * `python src/client_logic/fl_client.py client_B`
        * `python src/client_logic/fl_client.py client_C`
    5.  **Watch the logs** in your terminals for FL rounds progress.
    6.  **Visit your W&B dashboard** for real-time experiment tracking:
        [W&B Project Dashboard Link (UPDATE WITH YOURS!)](https://wandb.ai/YOUR_WANDB_USERNAME/guardian-ai-fl)
    """)
    st.warning("""
    *Note for live Streamlit deployment:* Running subprocesses directly in Streamlit on cloud platforms can be unstable. For the live demo, this section will explain the process and link to external logs (W&B).
    """)

    st.subheader("4. Layer 3: Decrypted & Auditable Insights")
    st.markdown("""
    *Only authorized personnel can decrypt and view high-level, actionable insights. Every decryption event is logged conceptually, ensuring full auditability and compliance.*
    """)
    st.subheader("Simulated Aggregated Insight Decryption")
    st.markdown("""
    *This demonstrates decrypting an aggregated score that was homomorphically summed across clients.*
    """)
    if st.button("Decrypt Sample Aggregated Risk Score"):
        if st.session_state.public_key and st.session_state.private_key:
            simulated_total_risk = random.uniform(0.1, 0.9)
            simulated_encrypted_aggregated_score = st.session_state.public_key.encrypt(simulated_total_risk)
            st.write(f"Simulated Encrypted Aggregated Score: `{simulated_encrypted_aggregated_score}`")
            decrypted_score = decrypt_value(simulated_encrypted_aggregated_score, st.session_state.private_key)
            st.success(f"**Decrypted Global Network Risk Score: {decrypted_score:.4f}**")
            st.write("""
            *This demonstrates that insights can be derived and aggregated while remaining encrypted,
            only being decrypted at the very last, controlled stage by an authorized entity.*
            """)
            st.info(f"""
            **Achieving "~99.99% Match Output":** This high accuracy refers to the utility of these *decrypted insights*.
            By leveraging robust feature extractors and well-trained models (even on privacy-preserved data)
            during the FL process, the derived risk assessments on the overall system remain highly accurate and actionable.
            Our [W&B dashboard](https://wandb.ai/YOUR_WANDB_USERNAME/guardian-ai-fl) shows the FL model's accuracy on a public test set.
            """)
        else:
            st.error("Global Paillier keys not initialized. Please refresh the page or restart the app.")
    st.subheader("Audit Trail (Conceptual)")
    st.markdown("""
    *Every sensitive operation and decryption event is logged for compliance and accountability.*
    """)
    st.code("""
    [2025-07-30 22:30:01] Client_A: Raw data generated.
    [2025-07-30 22:30:02] Client_A: Features extracted. Compliance score encrypted.
    [2025-07-30 22:30:05] Client_B: Raw data generated.
    [2025-07-30 22:30:06] Client_B: Features extracted. Compliance score encrypted.
    ...
    [2025-07-30 22:31:10] FL Server: Received encrypted model updates from 3 clients.
    [2025-07-30 22:31:15] FL Server: Performed homomorphic aggregation of model parameters.
    [2025-07-30 22:32:00] UI: Decryption of aggregated risk score by 'AuditorUser'. Result: 0.X
    """)

elif page_selection == "Zero-Trust Principles":
    st.header("Zero-Trust Principles in Guardian AI")
    st.markdown("""
    Guardian AI is built upon the **Zero-Trust Architecture** model, assuming no implicit trust.
    Every request and data interaction is verified and secured.
    #### 1. Eliminate Implicit Trust
    * **How Guardian AI does it:** Raw data is never trusted once it leaves its local enclave. It's immediately encrypted (HE) or handled via privacy-preserving mechanisms (FL). No network segment is inherently trusted.
    * **MLOps Application:** All data transfers (simulated) are conceptualized as encrypted channels.
    #### 2. Verify Explicitly
    * **How Guardian AI does it:** While we don't have user authentication in this demo, conceptually, decryption (Layer 3) would require explicit authorization and identity verification. Each client in FL verifies the global model it receives.
    * **MLOps Application:** Future integration with identity and access management (IAM) for model access and decryption keys.
    #### 3. Least Privilege Access
    * **How Guardian AI does it:** Clients only access their own raw data. The FL server only sees encrypted model updates, not raw data. Analysts only see aggregated, decrypted insights, not individual sensitive data points.
    * **MLOps Application:** Granular access controls for different stages of the MLOps pipeline and model artifacts.
    #### 4. Assume Breach Mentality
    * **How Guardian AI does it:** Even if a layer is compromised, the data at that layer is still encrypted or in a privacy-preserving form, minimizing damage.
    * **MLOps Application:** Robust logging (conceptual audit trail) and continuous monitoring for anomalous activities within the pipeline.
    #### 5. Continuously Monitor & Validate
    * **How Guardian AI does it:** Federated Learning continually updates the global model. Weights & Biases tracks performance across rounds.
    * **MLOps Application:** Automated validation tests, performance monitoring (W&B), and alerting for model drift or privacy budget exhaustion.
    ...
    st.subheader("Security Compliance & Responsible AI")
    st.markdown("""
    By implementing these principles and privacy-preserving techniques, Guardian AI aligns with key compliance standards:
    * **GDPR / CCPA:** By minimizing data exposure and enabling "right to be forgotten" (as raw data is never centralized).
    * **HIPAA:** Protecting sensitive health information through encryption and limited access.
    * **ISO 27001:** Demonstrates robust information security management practices.
    """)
    st.markdown("""
    This project embodies **Responsible AI** principles:
    * **Privacy & Security by Design:** Built-in from the ground up.
    * **Fairness (Conceptual):** By training on aggregated knowledge, models can potentially be more robust to bias in individual client datasets (though advanced fairness metrics are out of scope for this demo).
    * **Transparency & Explainability:** Through the layered architecture visualization and the conceptual audit trail.
    * **Accountability:** Clear logging of decryption events and MLOps processes.
    """)
